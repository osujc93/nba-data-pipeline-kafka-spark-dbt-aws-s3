# ===================
# spark-defaults.conf
# ===================

# ----------------------------------------------------------------------------
# Spark deploy mode, masters, etc.
# ----------------------------------------------------------------------------

# the cluster’s information will be stored in ZooKeeper
# Spark masters can recover cluster states and worker information after failures.
spark.deploy.recoveryMode=ZOOKEEPER

# ZooKeeper servers
spark.deploy.zookeeper.url=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181

# Directory path in ZooKeeper where Spark’s recovery info is stored
spark.deploy.zookeeper.dir=/spark

# Points to all the masters
# Failover will happen if one master is down.
spark.master=spark://spark-master1:7077,spark-master2:7077,spark-master3:7077

# Enables Spark event logging; writes run-time events for Spark applications
spark.eventLog.enabled=true

# Location where Spark event logs will be stored on s3.
spark.eventLog.dir=s3a://nelodatawarehouse93/spark-events

# ----------------------------------------------------------------------------
# Iceberg Catalog & Hive Metastore
# ----------------------------------------------------------------------------

# Iceberg-specific extensions
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Points the default 'spark_catalog' to Iceberg SparkCatalog 
spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.spark_catalog.type=hive
spark.sql.catalog.spark_catalog.warehouse=s3a://nelodatawarehouse93/warehouse

# use Hive-based catalog, which interacts with the Hive Metastore.
spark.sql.catalogImplementation=hive

# Location of Hive Metastore service
spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083

# ----------------------------------------------------------------------------
# RPC, Network, and UI ACLs
# ----------------------------------------------------------------------------

# Maximum allowed message size for RPC in bytes
spark.rpc.message.maxSize=512

# Global network timeout for various internal operations, set to 300 seconds.
spark.network.timeout=300000

# Defines ACLs for the Spark UI. Only these users/groups can view the UI.
spark.ui.view.acls=spark
spark.ui.view.acls.groups=hadoop

# Defines ACLs for modifying the Spark UI
spark.modify.acls=spark
spark.modify.acls.groups=hadoop

# ----------------------------------------------------------------------------
# Metrics Configuration
# ----------------------------------------------------------------------------

# Points Spark to the metrics configuration file for controlling metrics reporting.
spark.metrics.conf=/opt/spark/conf/metrics.properties

# Enables the appStatusSource for metrics, so we get application-level status metrics.
spark.metrics.appStatusSource.enabled=true

# Enable static sources that do not change at runtime
spark.metrics.staticSources.enabled=true

# Allows Spark to expose metrics in Prometheus format at /metrics/json.
spark.ui.prometheus.enabled=true

# ----------------------------------------------------------------------------
# Standalone Worker Resource Settings
# ----------------------------------------------------------------------------

# The number of CPU cores to allocate for each standalone Spark worker.
spark.worker.cores=1

# The total memory allocated for each Spark worker process.
spark.worker.memory=4g

# Network ports on which the worker and its web UI run.
spark.worker.port=7081
spark.worker.webui.port=8090

# Extra JVM options for the worker to specify a custom log4j configuration.
spark.worker.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties

# ----------------------------------------------------------------------------
# Serialization and Memory Usage
# ----------------------------------------------------------------------------

# Use Kryo for faster serialization than the default Java serializer.
spark.serializer=org.apache.spark.serializer.KryoSerializer

# When true, Spark enforces that all Kryo-serialized classes be registered for performance.
# Currently set to false to avoid registration overhead for unknown classes.
spark.kryo.registrationRequired=false

# Fraction of the (executor) heap space to use for execution/storage
spark.memory.fraction=0.7

spark.memory.storageFraction=0.2

spark.dynamicAllocation.enabled=false
spark.shuffle.service.enabled=false

# ----------------------------------------------------------------------------
# XGBoost Settings
# ----------------------------------------------------------------------------

# Which tree-building algorithm XGBoost uses (hist recommended for performance).
spark.xgboost.treeMethod=hist

# Number of XGBoost worker tasks to run in parallel.
spark.xgboost.numWorkers=3

# Number of threads per XGBoost task.
spark.xgboost.numThreads=1

spark.task.cpus=1

# ----------------------------------------------------------------------------
# Executor and Driver Settings
# ----------------------------------------------------------------------------

spark.executor.memory=3372m

# Additional off-heap or overhead memory for each executor
spark.executor.memoryOverhead=512m

# Number of CPU cores per executor. 
spark.executor.cores=1

spark.driver.memory=2g

# Additional overhead memory for the driver process.
spark.driver.memoryOverhead=512m

# Number of CPU cores on the driver.
spark.driver.cores=1

# Default parallelism (tasks) 
spark.default.parallelism=6

# Controls verbosity of event logs.
spark.eventLog.logLevel=DEBUG

# Maximum allowed size for serialized results returned by the driver. 
spark.driver.maxResultSize=1g

# Extra Java options for driver and executor, here enabling necessary reflective access
# for certain Spark/Java libraries, plus enabling G1GC.
spark.driver.extraJavaOptions=--add-opens=java.base/java.nio=ALL-UNNAMED \
 --add-opens=java.base/java.lang=ALL-UNNAMED \
 --add-opens=java.base/java.lang.reflect=ALL-UNNAMED \
 --add-opens=java.base/java.lang.invoke=ALL-UNNAMED \
 --add-opens=java.base/sun.nio.ch=ALL-UNNAMED \
 -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=70

spark.executor.extraJavaOptions=--add-opens=java.base/java.nio=ALL-UNNAMED \
 --add-opens=java.base/java.lang=ALL-UNNAMED \
 --add-opens=java.base/java.lang.reflect=ALL-UNNAMED \
 --add-opens=java.base/java.lang.invoke=ALL-UNNAMED \
 --add-opens=java.base/sun.nio.ch=ALL-UNNAMED \
 -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=70

# ----------------------------------------------------------------------------
# Adaptive Query Execution
# ----------------------------------------------------------------------------

spark.sql.adaptive.enabled=false

# Sets Iceberg’s target file size for each output file (100 MB here).
spark.sql.iceberg.target-file-size-bytes=104857600

spark.sql.autoBroadcastJoinThreshold=104857600

# ----------------------------------------------------------------------------
# Python Environments
# ----------------------------------------------------------------------------

# Specifies which Python executable to use inside Spark tasks and on the driver.
spark.pyspark.python=/usr/bin/python3
spark.pyspark.driver.python=/usr/bin/python3

# ----------------------------------------------------------------------------
# Fault Tolerance
# ----------------------------------------------------------------------------

# Maximum number of times a task can fail before giving up the job.
spark.task.maxFailures=8

# ----------------------------------------------------------------------------
# S3A Filesystem Settings
# ----------------------------------------------------------------------------

# Tells Hadoop/Spark to use the S3AFileSystem implementation for "s3a://" paths.
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# Defines how AWS credentials are provided; 
# here it reads from environment variables like AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY.
spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.EnvironmentVariableCredentialsProvider

# Disables path-style access if the S3 bucket uses virtual-host-based addressing.
spark.hadoop.fs.s3a.path.style.access=false

# Enables SSL for S3 connections; recommended for security.
spark.hadoop.fs.s3a.connection.ssl.enabled=true

# Connection timeout in milliseconds for S3.
spark.hadoop.fs.s3a.connection.timeout=300000

# Maximum number of retry attempts if an S3 operation fails.
spark.hadoop.fs.s3a.attempts.maximum=20

# Maximum number of listing results returned per page from S3.
spark.hadoop.fs.s3a.paging.maximum=1000

# Enables multi-object deletes, allowing faster deletion of multiple objects in one API call.
spark.hadoop.fs.s3a.multiobjectdelete.enable=true

# Faster uploads by buffering data in memory before sending to S3.
spark.hadoop.fs.s3a.fast.upload=true

# Disables "magic committer" (if you are not using S3 "magic" commit mode).
spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled=false
