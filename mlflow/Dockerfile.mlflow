# syntax=docker/dockerfile:1

FROM python:3.8-slim

WORKDIR /mlflow

# Install necessary packages
RUN apt-get update && apt-get install -y \
    libpq-dev \
    gcc \
    python3-dev \
    python3-pip \
    curl \
    vim \
    wget \
    unzip \
    libxml2-dev \
    libffi-dev \
    libssl-dev \
    openjdk-17-jdk \
    libsasl2-dev \
    && rm -rf /var/lib/apt/lists/*

# Define environment variables
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV SPARK_HOME=/opt/spark
ENV HADOOP_VERSION=3.4.0
ENV HADOOP_HOME=/usr/local/hadoop
ENV HADOOP_COMMON_HOME=/usr/local/hadoop
ENV HADOOP_HDFS_HOME=/usr/local/hadoop
ENV HADOOP_MAPRED_HOME=/usr/local/hadoop
ENV HADOOP_YARN_HOME=/usr/local/hadoop
ENV HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
ENV YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop
ENV LD_LIBRARY_PATH=/usr/local/hadoop/lib/native
ENV PATH=$PATH:$JAVA_HOME/bin:$SPARK_HOME/bin:$HADOOP_HOME/bin

RUN chmod +x $JAVA_HOME/bin/java

# Download and install Hadoop
RUN wget -O /tmp/hadoop-${HADOOP_VERSION}.tar.gz \
      "https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz" && \
    tar -xzf /tmp/hadoop-${HADOOP_VERSION}.tar.gz -C /usr/local && \
    mv /usr/local/hadoop-${HADOOP_VERSION} /usr/local/hadoop && \
    rm /tmp/hadoop-${HADOOP_VERSION}.tar.gz

# Create a user
RUN useradd -ms /bin/bash mluser

# Create logs directory and set ownership
RUN mkdir -p /usr/local/hadoop/logs \
    && chown -R mluser:mluser /usr/local/hadoop/logs

# Download and add necessary JAR files for Hadoop/Spark
RUN mkdir -p /opt/spark/jars && \
    wget -O /opt/spark/jars/hadoop-common-3.4.0.jar \
      "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.4.0/hadoop-common-3.4.0.jar" && \
    wget -O /opt/spark/jars/hadoop-yarn-common-3.4.0.jar \
      "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/3.4.0/hadoop-yarn-common-3.4.0.jar" && \
    wget -O /opt/spark/jars/hadoop-mapreduce-client-common-3.4.0.jar \
      "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/3.4.0/hadoop-mapreduce-client-common-3.4.0.jar" && \
    wget -O /opt/spark/jars/iceberg-spark-runtime-3.3_2.12-1.5.2.jar \
      "https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar" && \
    wget -O /opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar \
      "https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar" && \
    wget -O /opt/spark/jars/xgboost4j-spark_2.12-2.1.3.jar \
      "https://repo1.maven.org/maven2/ml/dmlc/xgboost4j-spark_2.12/2.1.3/xgboost4j-spark_2.12-2.1.3.jar" && \
    wget -O /opt/spark/jars/woodstox-core-7.1.0.jar \
      "https://repo1.maven.org/maven2/com/fasterxml/woodstox/woodstox-core/7.1.0/woodstox-core-7.1.0.jar" && \
    wget -O /opt/spark/jars/stax2-api-4.2.2.jar \
      "https://repo1.maven.org/maven2/org/codehaus/woodstox/stax2-api/4.2.2/stax2-api-4.2.2.jar" && \
    wget -O /opt/spark/jars/commons-configuration2-2.11.0.jar \
      "https://repo1.maven.org/maven2/org/apache/commons/commons-configuration2/2.11.0/commons-configuration2-2.11.0.jar"

# Upgrade pip
RUN python3 -m pip install --upgrade pip

# Install Python dependencies
RUN pip3 install \
    mlflow[extras]==2.7.1 \
    pyspark==3.5.1 \
    pyarrow==12.0.1 \
    hdfs==2.7.0 \
    PyHive[hive]>=0.6.5 \
    thrift \
    psutil \
    pynvml \
    psycopg2

# enable system metrics logging
ENV MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING=true
ENV MLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL=5
ENV MLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING=2

EXPOSE 5000

CMD ["/bin/sh", "-c", "mlflow server --host 0.0.0.0 --backend-store-uri \"${MLFLOW_BACKEND_STORE_URI}\" --default-artifact-root /mlflow/artifacts"]
